{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3baa8485-7120-4af9-97ba-84a9135772b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "blogs = [\n",
    "  (1, \"Jules\", \"Damji\",  \"1/4/2016\", 4535),\n",
    "  (2, \"Brooke\",\"Wenig\",  \"5/5/2018\", 8908),\n",
    "  (3, \"Denny\", \"Lee\",  \"6/7/2019\", 7659),\n",
    "  (4, \"Tathagata\", \"Das\", \"5/12/2018\", 10568),\n",
    "  (5, \"Matei\",\"Zaharia\",  \"5/14/2014\", 40578),\n",
    "  (6, \"Reynold\", \"Xin\",  \"3/2/2015\", 25568),\n",
    "  (7, \"Nicolas\", \"Fernandez\",  \"8/8/2012\", 9000),\n",
    "  (8, \"Ulises\", \"Pereyra\",  \"3/2/2011\", 7800),\n",
    "  (9, \"Sara\", \"Smith\", \"9/15/2020\", 6321),\n",
    "  (10, \"John\", \"Doe\", \"4/23/2017\", 9876),\n",
    "  (11, \"Alice\", \"Johnson\", \"7/10/2016\", 5432),\n",
    "  (12, \"Luis\", \"García\", \"2/14/2022\", 8743),\n",
    "  (13, \"Maria\", \"Lopez\", \"11/30/2019\", 6543),\n",
    "  (14, \"Carlos\", \"Martinez\", \"10/5/2018\", 9321),\n",
    "  (15, \"Laura\", \"Hernandez\", \"8/20/2017\", 7521),\n",
    "  (16, \"Pedro\", \"Gonzalez\", \"4/12/2016\", 6321),\n",
    "  (17, \"Ana\", \"Ramirez\", \"1/25/2015\", 5678),\n",
    "  (18, \"Diego\", \"Torres\", \"6/7/2014\", 9876),\n",
    "  (19, \"Valeria\", \"Perez\", \"3/18/2013\", 4567),\n",
    "  (20, \"Eduardo\", \"Fernandez\", \"9/9/2012\", 8743),\n",
    "  (21, \"Isabella\", \"Silva\", \"7/4/2011\", 3456),\n",
    "  (22, \"Miguel\", \"Ortega\", \"5/15/2010\", 7890),\n",
    "  (23, \"Luna\", \"Vargas\", \"12/28/2009\", 6543)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36d1adea-54b3-4eba-987e-2e75f961e0b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#1.\tDefinir un schema llamado schema_blogs (solo utilizar STRING e INT como tipos de dato) \n",
    "#    Columnas: Id, First, Last, Published, Hits\n",
    "\n",
    "# Es de buenas practicas poner True() para admitir nulos, aunque se puede obviar\n",
    "\n",
    "schema_blogs = StructType(\n",
    "                    [\n",
    "                      StructField(\"Id\", IntegerType(),True),\n",
    "                      StructField(\"First\", StringType(),True),\n",
    "                      StructField(\"Last\", StringType(),True),\n",
    "                      StructField(\"Published\", StringType(),True),\n",
    "                      StructField(\"Hits\", IntegerType(),True)\n",
    "                    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03f12a20-49e0-4138-aa8a-1cb711b267c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+---------+-----+\n| Id|    First|     Last|Published| Hits|\n+---+---------+---------+---------+-----+\n|  1|    Jules|    Damji| 1/4/2016| 4535|\n|  2|   Brooke|    Wenig| 5/5/2018| 8908|\n|  3|    Denny|      Lee| 6/7/2019| 7659|\n|  4|Tathagata|      Das|5/12/2018|10568|\n|  5|    Matei|  Zaharia|5/14/2014|40578|\n|  6|  Reynold|      Xin| 3/2/2015|25568|\n|  7|  Nicolas|Fernandez| 8/8/2012| 9000|\n|  8|   Ulises|  Pereyra| 3/2/2011| 7800|\n|  9|     Sara|    Smith|9/15/2020| 6321|\n| 10|     John|      Doe|4/23/2017| 9876|\n+---+---------+---------+---------+-----+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# 2.\tCargar un DataFrame llamado df_blogs a partir de la lista blogs\n",
    "\n",
    "df_blogs = spark.createDataFrame(blogs,schema_blogs)\n",
    "\n",
    "df_blogs.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03d9f905-34ad-4466-b6aa-50aa0bdd6060",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|     Last|\n+---------+\n|    DAMJI|\n|    WENIG|\n|      LEE|\n|      DAS|\n|  ZAHARIA|\n|      XIN|\n|FERNANDEZ|\n|  PEREYRA|\n|    SMITH|\n|      DOE|\n+---------+\nonly showing top 10 rows\n\n+---------+\n|     Last|\n+---------+\n|    DAMJI|\n|    WENIG|\n|      LEE|\n|      DAS|\n|  ZAHARIA|\n|      XIN|\n|FERNANDEZ|\n|  PEREYRA|\n|    SMITH|\n|      DOE|\n+---------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "#3.\tListar todos los apellidos del DataFrame df_blogs\n",
    "#   Convertir todas las letras a mayúsculas con la función upper\n",
    "\n",
    "df_blogs = df_blogs.withColumn('Last', F.upper(F.col('Last')))\n",
    "\n",
    "df_blogs.select('Last').show(10)\n",
    "\n",
    "# Otra forma adicional de cambiarlo sería usando una expresion ( exp() )\n",
    "\n",
    "df_blogs = df_blogs.withColumn('Last', F.expr('UPPER(Last)'))\n",
    "\n",
    "df_blogs.select('Last').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b591cc05-9e6a-4da4-b29c-18443688b68a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+---------+-----+\n| Id|    First|   Last|Published| Hits|\n+---+---------+-------+---------+-----+\n|  4|Tathagata|    DAS|5/12/2018|10568|\n|  5|    Matei|ZAHARIA|5/14/2014|40578|\n|  6|  Reynold|    XIN| 3/2/2015|25568|\n+---+---------+-------+---------+-----+\n\n+---+---------+-------+---------+-----+\n| Id|    First|   Last|Published| Hits|\n+---+---------+-------+---------+-----+\n|  4|Tathagata|    DAS|5/12/2018|10568|\n|  5|    Matei|ZAHARIA|5/14/2014|40578|\n|  6|  Reynold|    XIN| 3/2/2015|25568|\n+---+---------+-------+---------+-----+\n\n+---+---------+-------+---------+-----+\n| Id|    First|   Last|Published| Hits|\n+---+---------+-------+---------+-----+\n|  4|Tathagata|    DAS|5/12/2018|10568|\n|  5|    Matei|ZAHARIA|5/14/2014|40578|\n|  6|  Reynold|    XIN| 3/2/2015|25568|\n+---+---------+-------+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#4.\tListar las filas que tengan una cantidad de Hits mayor o igual a 10000\n",
    "\n",
    "df_blogs.where(\"Hits >= 10000\").show()\n",
    "\n",
    "# O alternativamente utilizar la funcion col\n",
    "\n",
    "df_blogs.where(F.col('Hits')>=10000).show()\n",
    "\n",
    "# U otra alternativa es usar filter\n",
    "\n",
    "df_blogs.filter(df_blogs.Hits >=10000).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c67d23e8-3833-4863-a8d2-9fc1c00443c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+---------+-----+-------------+\n| Id|    First|   Last|Published| Hits|PublishedDate|\n+---+---------+-------+---------+-----+-------------+\n|  1|    Jules|  DAMJI| 1/4/2016| 4535|   2016-01-04|\n|  2|   Brooke|  WENIG| 5/5/2018| 8908|   2018-05-05|\n|  3|    Denny|    LEE| 6/7/2019| 7659|   2019-06-07|\n|  4|Tathagata|    DAS|5/12/2018|10568|   2018-05-12|\n|  5|    Matei|ZAHARIA|5/14/2014|40578|   2014-05-14|\n+---+---------+-------+---------+-----+-------------+\nonly showing top 5 rows\n\n+---+---------+-------+---------+-----+-------------+\n| Id|    First|   Last|Published| Hits|PublishedDate|\n+---+---------+-------+---------+-----+-------------+\n|  1|    Jules|  DAMJI| 1/4/2016| 4535|   2016-01-04|\n|  2|   Brooke|  WENIG| 5/5/2018| 8908|   2018-05-05|\n|  3|    Denny|    LEE| 6/7/2019| 7659|   2019-06-07|\n|  4|Tathagata|    DAS|5/12/2018|10568|   2018-05-12|\n|  5|    Matei|ZAHARIA|5/14/2014|40578|   2014-05-14|\n+---+---------+-------+---------+-----+-------------+\nonly showing top 5 rows\n\n+---+---------+-------+---------+-----+-------------+\n| Id|    First|   Last|Published| Hits|PublishedDate|\n+---+---------+-------+---------+-----+-------------+\n|  1|    Jules|  DAMJI| 1/4/2016| 4535|     1/4/2016|\n|  2|   Brooke|  WENIG| 5/5/2018| 8908|     5/5/2018|\n|  3|    Denny|    LEE| 6/7/2019| 7659|     6/7/2019|\n|  4|Tathagata|    DAS|5/12/2018|10568|    5/12/2018|\n|  5|    Matei|ZAHARIA|5/14/2014|40578|    5/14/2014|\n+---+---------+-------+---------+-----+-------------+\nonly showing top 5 rows\n\nroot\n |-- Id: integer (nullable = true)\n |-- First: string (nullable = true)\n |-- Last: string (nullable = true)\n |-- Published: string (nullable = true)\n |-- Hits: integer (nullable = true)\n |-- PublishedDate: date (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# 5.  Agregar una nueva columna llamada PublishedDate de tipo Date\n",
    "#\t  Utilizar la columna Published de tipo String como input\n",
    "\n",
    "\n",
    "df_blogs = df_blogs.withColumn('PublishedDate', F.to_date('Published','M/d/yyyy'))\n",
    "df_blogs.show(5)\n",
    "\n",
    "# Alternativamente, otra solución al problema sería optar por utilizar una UDF()\n",
    "\n",
    "def formatear_date(dato):\n",
    "     return datetime.strptime(dato, \"%m/%d/%Y\").date()\n",
    "\n",
    "spark.udf.register(\"formatear_date\", formatear_date, DateType())\n",
    "df_blogs = df_blogs.withColumn(\"PublishedDate\", udf(lambda x: formatear_date(x), DateType())(F.col(\"Published\")))\n",
    "df_blogs.show(5)\n",
    "\n",
    "# Extra: Si deseo solo visualizarlo con el formato original, podría usar date_format()\n",
    "\n",
    "df_blogs.withColumn(\"PublishedDate\", F.date_format(F.col(\"PublishedDate\"), \"M/d/yyyy\")).show(5)\n",
    "\n",
    "\n",
    "df_blogs.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdb4a37b-0229-49dc-b9d3-a7b9ed5f2511",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n|Published|Etiqueta|\n+---------+--------+\n| 1/4/2016| Antiguo|\n| 5/5/2018|Reciente|\n| 6/7/2019|Reciente|\n|5/12/2018|Reciente|\n|5/14/2014| Antiguo|\n| 3/2/2015| Antiguo|\n| 8/8/2012| Antiguo|\n| 3/2/2011| Antiguo|\n|9/15/2020|Reciente|\n|4/23/2017|Reciente|\n+---------+--------+\nonly showing top 10 rows\n\n+---------+--------+\n|Published|Etiqueta|\n+---------+--------+\n| 1/4/2016| Antiguo|\n| 5/5/2018|Reciente|\n| 6/7/2019|Reciente|\n|5/12/2018|Reciente|\n|5/14/2014| Antiguo|\n| 3/2/2015| Antiguo|\n| 8/8/2012| Antiguo|\n| 3/2/2011| Antiguo|\n|9/15/2020|Reciente|\n|4/23/2017|Reciente|\n+---------+--------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "#6.\tCrear una función UDF donde me devuelva un valor de tipo String\n",
    "#\tUtilizar como input la columna Published\n",
    "#\tLa función evalúa que los registros <= 2016 devuelva la etiqueta \"Antiguo\", de lo contrario \"Reciente\"\n",
    "#\tMostrar el resultado\n",
    "\n",
    "def devolver_string(dato):\n",
    "    valores = dato.split('/')\n",
    "    year = int(valores[2])\n",
    "    if year<=2016:\n",
    "        return \"Antiguo\"\n",
    "    else:\n",
    "        return \"Reciente\"\n",
    "\n",
    "spark.udf.register(\"devolver_string\", devolver_string)\n",
    "etiquetaUDF = udf(lambda x: devolver_string(x))\n",
    "\n",
    "df_blogs_etiquetas = df_blogs.select(\"Published\", etiquetaUDF(F.col('Published')).alias('Etiqueta'))\n",
    "\n",
    "df_blogs_etiquetas.show(10)\n",
    "\n",
    "\n",
    "# Otra solución alternativa, siguiendo con la línea de utilizar UDF sería:\n",
    "\n",
    "@udf(returnType=StringType()) \n",
    "def devolver_string_2(dato):\n",
    "    try:\n",
    "        fecha = datetime.strptime(dato, \"%m/%d/%Y\")\n",
    "        year = int(fecha.year)\n",
    "        if year <= 2016:\n",
    "            return \"Antiguo\"\n",
    "        else:\n",
    "            return \"Reciente\"\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "df_blogs_etiquetas_2 = df_blogs.select(\"Published\", devolver_string_2(F.col('Published')).alias('Etiqueta'))\n",
    "\n",
    "df_blogs_etiquetas_2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8329c503-b921-4dde-b930-4a27fe767286",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "TRABAJO FINAL - PYSPARK - ROY RIOS",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
